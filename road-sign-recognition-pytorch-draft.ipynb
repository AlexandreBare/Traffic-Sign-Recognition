{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traffic Sign Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install neptune-client --quiet\n",
    "! pip install livelossplot --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "CUDA_LAUNCH_BLOCKING = \"1\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import collections\n",
    "\n",
    "from PIL import Image\n",
    "import torch, torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split, WeightedRandomSampler\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score, accuracy_score\n",
    "from matplotlib import style\n",
    "style.use('fivethirtyeight')\n",
    "\n",
    "import random\n",
    "seed = 42\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers.neptune import NeptuneLogger\n",
    "\n",
    "neptune_logger = NeptuneLogger(\n",
    "    api_key=\"...\",\n",
    "    project_name=\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '../input/gtsrb-german-traffic-sign'\n",
    "train_dir = '../input/gtsrb-german-traffic-sign/Train'\n",
    "test_dir = '../input/gtsrb-german-traffic-sign/Test'\n",
    "\n",
    "# To resize the images to 32x32x3\n",
    "IMAGE_HEIGHT = 32\n",
    "IMAGE_WIDTH = 32\n",
    "N_CHANNELS = 3 # R-G-B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = { 0:'Speed limit (20km/h)',\n",
    "            1:'Speed limit (30km/h)', \n",
    "            2:'Speed limit (50km/h)', \n",
    "            3:'Speed limit (60km/h)', \n",
    "            4:'Speed limit (70km/h)', \n",
    "            5:'Speed limit (80km/h)', \n",
    "            6:'End of speed limit (80km/h)', \n",
    "            7:'Speed limit (100km/h)', \n",
    "            8:'Speed limit (120km/h)', \n",
    "            9:'No passing', \n",
    "            10:'No passing veh over 3.5 tons', \n",
    "            11:'Right-of-way at intersection', \n",
    "            12:'Priority road', \n",
    "            13:'Yield', \n",
    "            14:'Stop', \n",
    "            15:'No vehicles', \n",
    "            16:'Veh > 3.5 tons prohibited', \n",
    "            17:'No entry', \n",
    "            18:'General caution', \n",
    "            19:'Dangerous curve left', \n",
    "            20:'Dangerous curve right', \n",
    "            21:'Double curve', \n",
    "            22:'Bumpy road', \n",
    "            23:'Slippery road', \n",
    "            24:'Road narrows on the right', \n",
    "            25:'Road work', \n",
    "            26:'Traffic signals', \n",
    "            27:'Pedestrians', \n",
    "            28:'Children crossing', \n",
    "            29:'Bicycles crossing', \n",
    "            30:'Beware of ice/snow',\n",
    "            31:'Wild animals crossing', \n",
    "            32:'End speed + passing limits', \n",
    "            33:'Turn right ahead', \n",
    "            34:'Turn left ahead', \n",
    "            35:'Ahead only', \n",
    "            36:'Go straight or right', \n",
    "            37:'Go straight or left', \n",
    "            38:'Keep right', \n",
    "            39:'Keep left', \n",
    "            40:'Roundabout mandatory', \n",
    "            41:'End of no passing', \n",
    "            42:'End no passing veh > 3.5 tons' }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Class Distribution (Training Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = os.listdir(train_dir)\n",
    "\n",
    "train_number = []\n",
    "class_num = []\n",
    "\n",
    "for folder in folders:\n",
    "    train_files = os.listdir(train_dir + '/' + folder)\n",
    "    train_number.append(len(train_files))\n",
    "    class_num.append(classes[int(folder)])\n",
    "    \n",
    "# Sorting the dataset on the basis of number of images in each class\n",
    "zipped_lists = zip(train_number, class_num)\n",
    "sorted_pairs = sorted(zipped_lists)\n",
    "tuples = zip(*sorted_pairs)\n",
    "train_number, class_num = [list(tuple) for tuple in tuples]\n",
    "\n",
    "# Plotting the number of images in each class\n",
    "plt.figure(figsize=(21,10))  \n",
    "plt.bar(class_num, train_number)\n",
    "plt.xticks(class_num, rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Random Samples (Testing Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing 25 random images from test data\n",
    "import random\n",
    "from matplotlib.image import imread\n",
    "\n",
    "test = pd.read_csv(root_dir + '/Test.csv')\n",
    "imgs = test[\"Path\"].values\n",
    "\n",
    "plt.figure(figsize=(25,25))\n",
    "\n",
    "for i in range(1,26):\n",
    "    plt.subplot(5,5,i)\n",
    "    random_img_path = root_dir + '/' + random.choice(imgs)\n",
    "    rand_img = imread(random_img_path)\n",
    "    plt.imshow(rand_img)\n",
    "    plt.grid(b=None)\n",
    "    plt.xlabel(rand_img.shape[1], fontsize = 20) # width of image\n",
    "    plt.ylabel(rand_img.shape[0], fontsize = 20) # height of image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset & Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GTSRBDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        \"\"\"Initializes a dataset containing images and labels.\"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.images = images \n",
    "        self.labels = labels\n",
    "                \n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the size of the dataset.\"\"\"\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Returns the index-th data item of the dataset.\"\"\"\n",
    "        image = self.images[index]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, self.labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(root_dir, train=True):\n",
    "    csv_file = [\"Test.csv\", \"Train.csv\"][train]\n",
    "    csv = pd.read_csv(root_dir + '/' + csv_file)\n",
    "    image_paths = csv[\"Path\"].values\n",
    "    image_labels = csv[\"ClassId\"].values\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    for image_path, image_label in zip(image_paths, image_labels):\n",
    "        image = Image.open(root_dir + '/' + image_path).convert('RGB')\n",
    "        images.append(image)\n",
    "        labels.append(image_label)\n",
    "    \n",
    "    return images, image_labels\n",
    "\n",
    "\n",
    "full_train_set = load_dataset(root_dir, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_size = 0.4\n",
    "train_images, validation_images, train_labels, validation_labels = train_test_split(full_train_set[0], \n",
    "                                                                                    full_train_set[1], \n",
    "                                                                                    test_size=validation_size, \n",
    "                                                                                    random_state=seed,\n",
    "                                                                                    stratify=full_train_set[1])\n",
    "print(f\"Trainset length: {len(train_images)}\")\n",
    "print(f\"Validation set length: {len(validation_images)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the train set and the validation set have the same distribution of classes\n",
    "import collections\n",
    "train_set_frequency = collections.Counter(sorted(train_labels))\n",
    "print({k: v/len(train_labels) for k, v in dict(train_set_frequency).items()})\n",
    "frequency = collections.Counter(sorted(validation_labels))\n",
    "print({k: v/len(validation_labels) for k, v in dict(frequency).items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(img):\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    npimg = img.numpy() * 0.5 + 0.5\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.grid(b=None)\n",
    "    plt.show()\n",
    "\n",
    "def show_sample_data_loader(loader):\n",
    "    dataiter = iter(loader)\n",
    "    images, labels = dataiter.next()\n",
    "    show_images(torchvision.utils.make_grid(images, nrow=5))\n",
    "    print('\\n'.join('%s' % classes[label] for label in labels.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation (Validation/Test Set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://androidkt.com/pytorch-image-augmentation-using-transforms/\n",
    "class AddGaussianNoise(object):\n",
    "    '''\n",
    "    Add gaussian noise\n",
    "    \n",
    "    Params:\n",
    "        mean: mean of the gaussian distribution\n",
    "        std:  standard deviation of the gaussian distribution\n",
    "    '''\n",
    "    def __init__(self, mean=0.0, std=1.0):\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "        \n",
    "    def __call__(self, tensor):\n",
    "        return tensor + torch.randn(tensor.size()) * self.std + self.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = transforms.Compose([transforms.Resize((IMAGE_HEIGHT, IMAGE_WIDTH)),\n",
    "                                    transforms.ToTensor(), \n",
    "                                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) \n",
    "\n",
    "validation_set = GTSRBDataset(validation_images, validation_labels, transform=test_transform)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=10, shuffle=True, num_workers=0)\n",
    "\n",
    "show_sample_data_loader(validation_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation (Train Set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply different random transformations to the train set images that should create additional coherent and plausible road sign images to train from. They may very well be found as such in the test set (eg: we should not apply a random horizontal flip because an important part of road signs won't have any meaning such as a reverse speed limit (50km/h) that won't be readable). We thus opted for a random combination of:\n",
    "- a random rotation and shear to cope for the different angles of view the image could have been taken from.\n",
    "- a random crop of 30% of the image size to cope for the fact that road signs may not always be completely present on the photographs.\n",
    "- a random color jitter to cope for reflection, ambient light, weather change, and potential lack of image quality coming from the camera.\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomComposeTransform(object):\n",
    "    '''\n",
    "    Randomly choose among a subset of transforms to apply\n",
    "    \n",
    "    Params:\n",
    "        transform_list: a list of transforms to apply in the given order\n",
    "    '''\n",
    "    def __init__(self, transform_list):\n",
    "        self.transform_list = transform_list\n",
    "        \n",
    "    def __call__(self, tensor):\n",
    "        transform_list_len = len(self.transform_list) \n",
    "        random_sorted_indexes = sorted(random.sample(range(transform_list_len), k=random.randint(0, transform_list_len)))\n",
    "        if not random_sorted_indexes:\n",
    "            return transforms.RandomAffine(degrees=0)(tensor) # identity transform <-> applies no transformation\n",
    "        return transforms.Compose([self.transform_list[i] for i in random_sorted_indexes])(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_fraction = 0.3\n",
    "random_crop_transform = transforms.Compose([transforms.Resize((round(IMAGE_HEIGHT * (1 + crop_fraction)), round(IMAGE_WIDTH * (1 + crop_fraction)))),\n",
    "                                            transforms.RandomCrop((IMAGE_HEIGHT, IMAGE_WIDTH))])\n",
    "random_compose_transforms = RandomComposeTransform([transforms.RandomAffine(degrees=10, shear=40),\n",
    "                                                    random_crop_transform,\n",
    "                                                    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05)\n",
    "                                                    #AddGaussianNoise(0.1, 0.05),\n",
    "                                                   ])\n",
    "train_transform = transforms.Compose([#transforms.Resize((IMAGE_HEIGHT, IMAGE_WIDTH)),\n",
    "                                      random_compose_transforms,\n",
    "                                      #AddGaussianNoise(0.1, 0.05),\n",
    "                                      transforms.Resize((IMAGE_HEIGHT, IMAGE_WIDTH)),\n",
    "                                      transforms.ToTensor(), \n",
    "                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "train_set = GTSRBDataset(train_images, train_labels, transform=train_transform) # training dataloader\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=10, shuffle=True, num_workers=0)\n",
    "\n",
    "show_sample_data_loader(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train - Validation Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_labels_frequency = torch.tensor(list(collections.Counter(train_labels).values()))\n",
    "#weighted_sampler = WeightedRandomSampler(train_labels_frequency[torch.tensor(train_labels)], len(train_labels), replacement=True)\n",
    "\n",
    "# Training dataloader\n",
    "train_set = GTSRBDataset(train_images, train_labels, transform=train_transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=100, shuffle=True, num_workers=50)\n",
    "#train_loader = torch.utils.data.DataLoader(train_set, batch_size=100, shuffle=False, num_workers=50, sampler=weighted_sampler)\n",
    "\n",
    "# Validation dataloader\n",
    "validation_set = GTSRBDataset(validation_images, validation_labels, transform=test_transform)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=100, shuffle=False, num_workers=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(next(iter(train_loader)).shape)\n",
    "batch_iter = iter(train_loader)\n",
    "image, labels = batch_iter.next()\n",
    "\n",
    "print(labels.tolist())\n",
    "train_batch_frequency = collections.Counter(sorted(labels.tolist()))\n",
    "print({k: v/100 for k, v in dict(train_batch_frequency).items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the output size of a conv layer\n",
    "def outputSize(in_size, kernel_size, stride=1, padding=1):\n",
    "  output = int((in_size - kernel_size + 2*(padding)) / stride) + 1\n",
    "  return(output)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputSize(13, 2, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = input_dim\n",
    "BN1 = nn.BatchNorm2d(input_dim)\n",
    "conv1 = nn.Conv2d(input_dim, conv1_output_channels, kernel_size=3, stride=1, padding=0)\n",
    "pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=1)\n",
    "BN2 = nn.BatchNorm2d(conv1_output_channels)\n",
    "conv2 = nn.Conv2d(conv1_output_channels, conv2_output_channels, kernel_size=3, stride=1, padding=0)\n",
    "BN3 = nn.BatchNorm2d(conv2_output_channels)\n",
    "conv3 = nn.Conv2d(conv2_output_channels, last_conv_output_channels, kernel_size=3, stride=1, padding=0)\n",
    "\n",
    "BN4 = nn.BatchNorm2d(last_conv_output_channels)\n",
    "fc1_input_channels = last_conv_output_channels * last_conv_output_dim * last_conv_output_dim\n",
    "fc1 = nn.Linear(fc1_input_channels, fc1_input_channels)\n",
    "fc2 = nn.Linear(fc1_input_channels, output_dim)\n",
    "\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "x = images\n",
    "print(x.shape)\n",
    "# Block 1\n",
    "x = BN1(x)\n",
    "x = F.relu(conv1(x))\n",
    "x = pool(x)\n",
    "print(x.shape)\n",
    "# Block 2\n",
    "x = BN2(x)\n",
    "x = F.relu(conv2(x))\n",
    "x = pool(x)\n",
    "print(x.shape)\n",
    "# Block 3\n",
    "x = BN3(x)\n",
    "x = F.relu(conv3(x))\n",
    "x = pool(x)\n",
    "print(x.shape)\n",
    "# Block 4\n",
    "x = BN4(x)\n",
    "x = x.view(-1, last_conv_output_channels * last_conv_output_dim * last_conv_output_dim)\n",
    "x = F.relu(fc1(x))\n",
    "x = fc2(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers.neptune import NeptuneLogger\n",
    "from livelossplot import PlotLosses\n",
    "\n",
    "neptune_logger = NeptuneLogger(\n",
    "    api_key=\"...\",\n",
    "    project_name=\"...\")\n",
    "#liveplot = PlotLosses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install efficientnet_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from efficientnet_pytorch import EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testmodel = EfficientNet.from_pretrained('efficientnet-b4')\n",
    "print(testmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 3 \n",
    "conv1_output_channels = 32\n",
    "conv2_output_channels = 64\n",
    "last_conv_output_channels = 128\n",
    "last_conv_output_dim = 4\n",
    "output_dim = len(classes)\n",
    "learning_rate = 0.001\n",
    "max_epochs = 100\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "class CNN(pl.LightningModule): #nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        # self.input_dim = input_dim\n",
    "        # self.BN1 = nn.BatchNorm2d(input_dim)\n",
    "        # self.conv1 = nn.Conv2d(input_dim, conv1_output_channels, kernel_size=3, stride=1, padding=0)\n",
    "        # self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=1)\n",
    "        # self.BN2 = nn.BatchNorm2d(conv1_output_channels)\n",
    "        # self.conv2 = nn.Conv2d(conv1_output_channels, conv2_output_channels, kernel_size=3, stride=1, padding=0)\n",
    "        # self.BN3 = nn.BatchNorm2d(conv2_output_channels)\n",
    "        # self.conv3 = nn.Conv2d(conv2_output_channels, last_conv_output_channels, kernel_size=3, stride=1, padding=0)\n",
    "        \n",
    "        # self.BN4 = nn.BatchNorm2d(last_conv_output_channels)\n",
    "        # fc1_input_channels = last_conv_output_channels * last_conv_output_dim * last_conv_output_dim\n",
    "        self.base = EfficientNet.from_pretrained('efficientnet-b4')\n",
    "        for param in self.base.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        num_features = self.base._fc.in_features\n",
    "        self.base._fc = nn.Linear(num_features, output_dim)\n",
    "        # self.fc1 = nn.Linear(fc1_input_channels, fc1_input_channels)\n",
    "        # self.fc2 = nn.Linear(fc1_input_channels, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # # Block 1\n",
    "        # x = self.BN1(x)\n",
    "        # x = F.relu(self.conv1(x))\n",
    "        # x = self.pool(x)\n",
    "        \n",
    "        # # Block 2\n",
    "        # x = self.BN2(x)\n",
    "        # x = F.relu(self.conv2(x))\n",
    "        # x = self.pool(x)\n",
    "\n",
    "        # # Block 3\n",
    "        # x = self.BN3(x)\n",
    "        # x = F.relu(self.conv3(x))\n",
    "        # x = self.pool(x)\n",
    "        \n",
    "        # # Block 4\n",
    "        # x = self.BN4(x)\n",
    "        # x = x.view(-1, last_conv_output_channels * last_conv_output_dim * last_conv_output_dim)\n",
    "        # x = F.relu(self.fc1(x))\n",
    "        # x = self.fc2(x)\n",
    "        x = self.base(x)\n",
    "        return x\n",
    "    \n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        x, y = train_batch\n",
    "        y_hat = self.forward(x)\n",
    "        loss = criterion(y_hat, y)\n",
    "        #self.log('train_loss', loss)\n",
    "        y_hat = torch.argmax(y_hat, dim=1)#.cpu()\n",
    "        #accuracy = accuracy_score(y.cpu(), y_hat)\n",
    "        #balanced_accuracy = balanced_accuracy_score(y.cpu(), y_hat)\n",
    "        accuracy = torch.sum(y == y_hat).item() / len(y)\n",
    "        #self.log('train_accuracy', accuracy)\n",
    "        logs = {'train_loss': loss, 'train_accuracy': accuracy}#, 'train_balanced_accuracy': balanced_accuracy}\n",
    "        self.log_dict(logs, on_epoch=True)\n",
    "        #liveplot.update(logs)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        x, y = val_batch\n",
    "        y_hat = self.forward(x)\n",
    "        loss = criterion(y_hat, y)\n",
    "        #self.log('val_loss', loss)\n",
    "        y_hat = torch.argmax(y_hat, dim=1)#.cpu()\n",
    "        #accuracy = accuracy_score(y.cpu(), y_hat)\n",
    "        #balanced_accuracy = balanced_accuracy_score(y.cpu(), y_hat)\n",
    "        accuracy = torch.sum(y == y_hat).item() / len(y)\n",
    "        #self.log('val_accuracy', accuracy)\n",
    "        logs = {'val_loss': loss, 'val_accuracy': accuracy}#, 'val_balanced_accuracy': balanced_accuracy}\n",
    "        self.log_dict(logs)\n",
    "        #liveplot.update(logs)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, val_batch, batch_idx):\n",
    "        x, y = val_batch\n",
    "        y_hat = self.forward(x)\n",
    "        loss = criterion(y_hat, y)\n",
    "        #self.log('test_loss', loss)\n",
    "        y_hat = torch.argmax(y_hat, dim=1)#.cpu()\n",
    "        #accuracy = accuracy_score(y, y_hat)\n",
    "        #balanced_accuracy = balanced_accuracy_score(y, y_hat)\n",
    "        accuracy = torch.sum(y == y_hat).item() / len(y)\n",
    "        #self.log('test_accuracy', accuracy)\n",
    "        logs = {'test_loss': loss, 'test_accuracy': accuracy}#, 'test_balanced_accuracy': balanced_accuracy}\n",
    "        self.log_dict(logs)\n",
    "        #liveplot.update(logs)\n",
    "        return loss\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "    \n",
    "#device = 'cuda:0'\n",
    "model = CNN(input_dim, output_dim)#.to(device)\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop_callback = EarlyStopping(\n",
    "   monitor='val_accuracy',#'val_balanced_accuracy',\n",
    "   min_delta=0.001,\n",
    "   patience=8,\n",
    "   verbose=True,\n",
    "   mode='max'\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(gpus=1, max_epochs=max_epochs, callbacks=[early_stop_callback], logger=neptune_logger)\n",
    "trainer.fit(model, train_loader, validation_loader)\n",
    "#liveplot.send()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images, test_labels = load_dataset(root_dir, train=False)\n",
    "print(f\"Trainset shape: {len(full_train_set[0])}\")\n",
    "print(f\"Test set shape: {len(test_images)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_train_set = GTSRBDataset(full_train_set[0], full_train_set[1], transform=train_transform)\n",
    "#full_train_loader = torch.utils.data.DataLoader(full_train_set, batch_size=100, shuffle=True, num_workers=50)\n",
    "test_set = GTSRBDataset(test_images, test_labels, transform=test_transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=100, shuffle=False, num_workers=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model, test_dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "model.eval()\n",
    "pred_labels = []\n",
    "model.to('cpu')\n",
    "wrong_dict = {\"pred_labels\":[], \"true_labels\":[], \"images\":[]}\n",
    "for test_image, test_label in test_set:\n",
    "    pred_label = torch.argmax(model(test_image.unsqueeze(0))).item()\n",
    "    pred_labels.append(pred_label)\n",
    "    \n",
    "    if pred_label != test_label: # if wrongly classified\n",
    "        wrong_dict[\"pred_labels\"].append(pred_label)\n",
    "        wrong_dict[\"true_labels\"].append(test_label)\n",
    "        wrong_dict[\"images\"].append(np.transpose(test_image, (1, 2, 0)))\n",
    "    \n",
    "cf = confusion_matrix(test_labels, pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_wrong_images = len(wrong_dict)\n",
    "print(f\"Number of wrongly classified test images: {n_wrong_images}\")\n",
    "n_cols = 10\n",
    "img_size = 2.5\n",
    "i = 0\n",
    "plt.figure(figsize=(round(img_size * n_wrong_images / n_cols), img_size * n_cols))\n",
    "for image in wrong_dict[\"images\"]:\n",
    "    plt.subplot(n_wrong_images/n_cols, n_cols, i)\n",
    "    i += 1\n",
    "    plt.imshow(image) \n",
    "    plt.grid(b=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "df_cm = pd.DataFrame(cf, index=classes,  columns=classes)\n",
    "plt.figure(figsize = (20,20))\n",
    "sns.heatmap(df_cm, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs, trainloader, testloader):\n",
    "    epochs_train_loss = []\n",
    "    epochs_test_loss = []\n",
    "    epochs_train_accuracy = []\n",
    "    epochs_test_accuracy = []\n",
    "    for i in range(num_epochs):\n",
    "        print(i)\n",
    "        if i % 1 == 0: # no use...\n",
    "            with torch.no_grad():\n",
    "                correct = 0\n",
    "                total = 0\n",
    "                tmp_test_loss = []\n",
    "                for inputs, targets in testloader:\n",
    "                    outputs = model(inputs.to(device))\n",
    "                    loss = criterion(outputs, targets.to(device))\n",
    "                    tmp_test_loss.append(loss.detach())\n",
    "                    _, predicted = outputs.max(1)\n",
    "                    total += targets.size(0)\n",
    "                    correct += predicted.eq(targets.to(device)).sum().detach() #.item() would transfer on CPU while .detach() works on GPU\n",
    "            epochs_test_loss.append(torch.mean(torch.stack(tmp_test_loss)))\n",
    "            accuracy = 100 * correct / total\n",
    "            epochs_test_accuracy.append(accuracy)\n",
    "            print('Accuracy of the model on the testing images: %f %%' % accuracy)\n",
    "        \n",
    "        correct = 0\n",
    "        total = 0\n",
    "        tmp_train_loss = []\n",
    "        for (x, y) in trainloader:\n",
    "            outputs = model(x.to(device))\n",
    "            loss = criterion(outputs, y.to(device))\n",
    "            tmp_train_loss.append(loss.detach())\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += y.size(0)\n",
    "            correct += predicted.eq(y.to(device)).sum().detach()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        epochs_train_loss.append(torch.mean(torch.stack(tmp_train_loss)))\n",
    "        accuracy = 100 * correct / total\n",
    "        epochs_train_accuracy.append(accuracy)\n",
    "        print('Accuracy of the model on the training images: %f %%' % accuracy)\n",
    "\n",
    "    return epochs_train_loss, epochs_test_loss, epochs_train_accuracy, epochs_test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Losses and Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(train_loss = None, test_loss = None, train_accuracy = None, test_accuracy = None, validation = False):\n",
    "    testing_string = [\"Testing\", \"Validation\"][validation]\n",
    "    plt.figure(figsize=(14, 4))\n",
    "    if train_loss and test_loss:\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.title('Training Loss')\n",
    "        plt.plot(train_loss)\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('(Cross Entropy) Loss')\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.title(testing_string + ' Loss')\n",
    "        plt.plot(epochs_test_loss)\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('(Cross Entropy) Loss')\n",
    "    \n",
    "    if train_accuracy and test_accuracy:\n",
    "        plt.figure(figsize=(14, 4))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.title('Training Accuracy')\n",
    "        plt.plot(epochs_train_accuracy)\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.title(testing_string + ' Accuracy')\n",
    "        plt.plot(epochs_test_accuracy)\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model (Train & Validation Sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_train_loss, epochs_validation_loss, epochs_train_accuracy, epochs_validation_accuracy = train(max_epochs, train_loader, validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(epochs_train_loss, epochs_validation_loss, epochs_train_accuracy, epochs_validation_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train - Test Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images, test_labels = load_dataset(root_dir, train=False)\n",
    "print(f\"Trainset shape: {len(full_train_set[0])}\")\n",
    "print(f\"Test set shape: {len(test_images)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_set = GTSRBDataset(full_train_set[0], full_train_set[1], transform=train_transform)\n",
    "full_train_loader = torch.utils.data.DataLoader(full_train_set, batch_size=100, shuffle=True, num_workers=50)\n",
    "\n",
    "test_set = GTSRBDataset(test_images, test_labels, transform=test_transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=100, shuffle=False, num_workers=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model (Train & Test Sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_train_loss, epochs_test_loss, epochs_train_accuracy, epochs_test_accuracy = train(num_epochs, train_loader, validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(epochs_train_loss, epochs_test_loss, epochs_train_accuracy, epochs_test_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
